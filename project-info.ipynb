{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Stock Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project will implement a big data analytics representation of various sectors of the stock market through an economic analysis of large data obtained from 1996-2020. Various aspects of the stock market provide an influx of information and data, from analysis of P/E ratio, dividends and alternative aspects of portfolio diversification through these attributes of stocks. The unpredictability of market behavior, from the previous years expedition of GameStop & AMC have been influenced by random investor behavior. This project will predict the values of the attributes of stock data based ond data obtained from Kaggle, Yahoo Finance, and Google, along with a business perspective in the prediction of variation of the stock market based on this data. Using this implementation will take a different approach on analyzing stock data from a daily perspective of its highs and lows, and instead measure the variability of its attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrastructure\n",
    "\n",
    "The primary infrastructure of this project will be using tools to both clean and make predictions on this dataset:\n",
    "\n",
    "* Python\n",
    "    * Python will be the primary method of choice to use when programming the assignment\n",
    "    * When cleaning up larger datasets, Python will be used to discover variables that may not contribute valuable data to the analysis\n",
    "    \n",
    "* Jupyter Notebook\n",
    "    * Jupyter Notebook will be the primary source of data analytics and cleaning. \n",
    "    \n",
    "* Github\n",
    "    * Github will be used for source control within this project\n",
    "    \n",
    "* Spark\n",
    "    * Spark will be primarily used when cleaning up the dataset, as most data obtained may not be in the format desired\n",
    "    * Spark will also be used to directly load data in a .csv or .tsv\n",
    "        * The latter contributes for better data visualiztion and understanding through the cleaning of the large datasets\n",
    "\n",
    "* Libraries in Python\n",
    "    * Libraries in Python (numpy, pandas, etc.) will also be utilized to ensure that we exhibit understanding of how to use these tools for big data analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources & Field Attributes\n",
    "\n",
    "The primary Data Source that is going to be used for this project will be the data obtained from Kaggle, which houses a collection of data obtained from the years 1960-2020. There are a variety of stocks and etfs that are within the stock market, but the main attributes that we will focus on to determine value are as follows:\n",
    "\n",
    "* Open:\n",
    "* High:\n",
    "* Low:\n",
    "* Close:\n",
    "* Adjusted Volume:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Analysis vs. Fundamental Analysis\n",
    "\n",
    "Economic data follows two fundamental analysis when analyzing the value of a stock. The first is technical analysis, which houses the infrastructure for understanding a time series model, and the second being fundamental analysis, which houses the general information regarding the stock in its relation to other components that may affect it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of the Big Data Analytics in Stock Market Anlysis\n",
    "\n",
    "In order to build a relevant pipeline for big data analysis of stocks, we focus on the three subsets that entail it: \n",
    "\n",
    "1. Data Processing\n",
    "    * The operations on the data in order to clean and ensure that the data we collect is applicable to what we wish to study.\n",
    "2. Data Analysis\n",
    "    * Once the data is cleaned, analyze it to determine and glean valuable information from it\n",
    "3. Data Visualization\n",
    "    * Visualize the data in a manner that allows for clear understanding of what is being told through numbers and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The purpose of analyzing this stock market data is to determine whether or not specific models applied to time series in stock market analysis are scalable. The steps to be taken with data processing are as follows:\n",
    "\n",
    "1. Since the dataset being worked with is fairly large, entailing data of all stocks from the years 1960's - 2000's, it is important to test a small portion of it, and determine whether this is scalable. \n",
    "    * For the purpose of the prototype, we will single this out to the years 2000 - 2005 instead, then work on various machine learning models to determine whether the prototype that we have created is scalable. \n",
    "2. To test for this method, the first primary analysis would be to select prototypes. This means that at minimum, 5 - 10 stocks will be prototyped in order to apply to a larger dataset. \n",
    "    * This small prototype will also determine why simple models are not the best in analyzing stocks, and why it is important to actually test these numbers using fundamental models instead. \n",
    "3. Once the prototype data has been tested and processed, and the fundamental algorithms to apply to the bigger dataset have been applied, application of this code should be applied to the stocks within each of the files for scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Data analysis would be the information that is generally obtained from the data collected and cleaned. The following steps augment the reasoning behind the data analysis, once the prototypes have been tested and scaled. It will be written for a subsample of the dataset, and determined whether this is scalable for the rest of the years. \n",
    "\n",
    "The steps to be taken for the portion of data analysis are as follows:\n",
    "1. Once the subsample of the data has been obtained, the code written for the prototype will be applied to this sample. \n",
    "    * It is noted that this sample must be run on years where trends are observed to be consecutive. This means that a random analysis of any given stock or year cannot be computed. \n",
    "    * For this particular data analysis portion, the years that are chosen are 2000 - 2005 consecutive, since under fundamental analysis of stocks, ther eare generally no factors that may have affected it in the major years.\n",
    "    * No great anomolies happened between these years, and this is a subsample that accounts for 1/10 of the original dataset. \n",
    "2. Data Analysis will focus on Time Series and Clustering of the Time Series\n",
    "    * Deep Learning and understanding what these models are doing in a time series analysis will be determined.\n",
    "    * To show that Linear Regression and LSTM is not the primary model that will be used, we will be using this model to show that it is not the best method of predicting stock prices.\n",
    "3. A better method of understanding the data is to apply it as if this were a stock portfolio. In order to do this, we can apply binning to a variety of stocks and determine which one of them prove to be a better portfolio, or which stocks or domains/sectors have the most variety. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "The data visualization will be primarily shown in Jupyter Notebook and Dash.py. The general concept will be plotting the bins, and selecting a representative sequence of the bines that has a couple of tie series. One of the stocks can then be plotted, and compared by different colors that are representative of the time series. \n",
    "\n",
    "From the data visualization, the goals to be obtained are as follows:\n",
    "1. Plotting of the bins\n",
    "2. Selecting a representative sequence that has a tie series and plot the trend\n",
    "3. Discover the trend in different bins\n",
    "4. Do a linear regression analysis on this, and see whether it is a stable bin\n",
    "5. Determine what domains or sectors have a lot of variety/most diversyt that aren't too complicated, and focus on the subsets of this. \n",
    "\n",
    "It is noted that the subset according to the Global Industry Classification Standard (GICS) can be applied to the clusters created. The list of GICS classifications are as follows:\n",
    "* Energy\n",
    "* Materials\n",
    "* Industrials\n",
    "* Consumer Discretionary Sectory\n",
    "* Consumer Staples Sector\n",
    "* Health Care Sector\n",
    "* Financials Sectory\n",
    "* Information Technology Sectory\n",
    "* Communication Services Sectory\n",
    "* Utilities Sectory\n",
    "* Real Estate Sector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
